#!/usr/bin/env python3
"""
Light EDA for all datasets referenced in the Geo-Sentimen UMKM (Pasarint) proposal.
Outputs summary statistics and sample inspection for each dataset.
"""
import os
import csv
from collections import Counter
from pathlib import Path

BASE = Path("/home/parasite/Work/UGM/data")

# â”€â”€â”€ 1. IndoLEM NER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("=" * 80)
print(" 1. IndoLEM NER Dataset (UGM + UI)")
print("=" * 80)

for corpus in ["nerugm", "nerui"]:
    corpus_path = BASE / "indolem_ner/indolem/ner/data" / corpus
    if not corpus_path.exists():
        print(f"  âš  {corpus} not found")
        continue

    print(f"\n  â”€â”€ {corpus.upper()} â”€â”€")

    # Aggregate across all fold files
    total_tokens = 0
    total_sentences = 0
    tag_counts = Counter()
    entity_types = Counter()
    samples = []

    for f in sorted(corpus_path.glob("*.tsv")):
        in_sentence = False
        sent_tokens = []
        with open(f, "r", encoding="utf-8") as fh:
            for line in fh:
                line = line.rstrip("\n")
                if line == "":
                    if sent_tokens:
                        total_sentences += 1
                        if len(samples) < 3:
                            samples.append(list(sent_tokens))
                        sent_tokens = []
                    continue
                parts = line.split("\t")
                if len(parts) >= 2:
                    token, tag = parts[0], parts[1]
                    total_tokens += 1
                    tag_counts[tag] += 1
                    if tag.startswith("B-"):
                        entity_types[tag[2:]] += 1
                    sent_tokens.append((token, tag))
            # last sentence
            if sent_tokens:
                total_sentences += 1
                sent_tokens = []

    print(f"  Files: {len(list(corpus_path.glob('*.tsv')))} TSV files (5 folds Ã— train/dev/test)")
    print(f"  Total tokens: {total_tokens:,}")
    print(f"  Total sentences: {total_sentences:,}")
    print(f"\n  Tag distribution:")
    for tag, count in tag_counts.most_common():
        pct = count / total_tokens * 100
        print(f"    {tag:20s}  {count:>8,}  ({pct:5.1f}%)")
    print(f"\n  Entity types (B- tag count):")
    for etype, count in entity_types.most_common():
        print(f"    {etype:20s}  {count:>6,}")
    print(f"\n  Sample sentence:")
    if samples:
        for tok, tag in samples[0][:15]:
            print(f"    {tok:20s}  {tag}")
        if len(samples[0]) > 15:
            print(f"    ... ({len(samples[0])} tokens total)")


# â”€â”€â”€ 2. NusaX Sentiment (Indonesian subset) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\n" + "=" * 80)
print(" 2. NusaX Sentiment Dataset (Indonesian subset)")
print("=" * 80)

nusax_path = BASE / "nusax_sentiment/nusax/datasets/sentiment/indonesian"
splits = {}
label_total = Counter()

for split in ["train", "valid", "test"]:
    fpath = nusax_path / f"{split}.csv"
    if not fpath.exists():
        continue
    rows = []
    with open(fpath, "r", encoding="utf-8") as fh:
        reader = csv.DictReader(fh)
        for row in reader:
            rows.append(row)
    splits[split] = rows

    labels = Counter(r["label"] for r in rows)
    label_total.update(labels)

    text_lengths = [len(r["text"].split()) for r in rows]
    avg_len = sum(text_lengths) / len(text_lengths) if text_lengths else 0
    min_len = min(text_lengths) if text_lengths else 0
    max_len = max(text_lengths) if text_lengths else 0

    print(f"\n  â”€â”€ {split.upper()} split â”€â”€")
    print(f"  Samples: {len(rows)}")
    print(f"  Label distribution:")
    for lbl, cnt in labels.most_common():
        print(f"    {lbl:12s}  {cnt:>5}  ({cnt/len(rows)*100:.1f}%)")
    print(f"  Text length (words): min={min_len}, avg={avg_len:.0f}, max={max_len}")

print(f"\n  â”€â”€ TOTAL across all splits â”€â”€")
total_samples = sum(len(v) for v in splits.values())
print(f"  Total samples: {total_samples}")
for lbl, cnt in label_total.most_common():
    print(f"    {lbl:12s}  {cnt:>5}  ({cnt/total_samples*100:.1f}%)")

# Class balance check
if label_total:
    values = list(label_total.values())
    ratio = max(values) / min(values) if min(values) > 0 else float("inf")
    print(f"\n  Class imbalance ratio (max/min): {ratio:.2f}")
    if ratio > 2:
        print("  âš  Significant class imbalance detected!")
    else:
        print("  âœ“ Classes are relatively balanced")

# Sample data
print(f"\n  Sample texts:")
if "train" in splits:
    for i, row in enumerate(splits["train"][:3]):
        text_preview = row["text"][:120] + ("..." if len(row["text"]) > 120 else "")
        print(f"    [{row['label']:>8s}] {text_preview}")


# â”€â”€â”€ 3. NusaX - All Languages Overview â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\n" + "=" * 80)
print(" 3. NusaX Sentiment - All Languages Overview")
print("=" * 80)

nusax_base = BASE / "nusax_sentiment/nusax/datasets/sentiment"
lang_counts = {}
for lang_dir in sorted(nusax_base.iterdir()):
    if lang_dir.is_dir():
        train_file = lang_dir / "train.csv"
        if train_file.exists():
            with open(train_file, "r", encoding="utf-8") as fh:
                reader = csv.reader(fh)
                rows = sum(1 for _ in reader) - 1  # minus header
            lang_counts[lang_dir.name] = rows

print(f"  Languages available: {len(lang_counts)}")
for lang, count in sorted(lang_counts.items()):
    print(f"    {lang:15s}  {count:>5} train samples")


# â”€â”€â”€ 4. Data Summary for Proposal â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("\n" + "=" * 80)
print(" 4. SUMMARY FOR PROPOSAL")
print("=" * 80)

print("""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Dataset                â”‚ Source       â”‚ Samples    â”‚ Labels/Tags    â”‚ Type â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤""")

# NER stats
for corpus in ["nerugm", "nerui"]:
    corpus_path = BASE / "indolem_ner/indolem/ner/data" / corpus
    tokens = 0
    sents = 0
    etypes = set()
    for f in corpus_path.glob("*.tsv"):
        with open(f, "r", encoding="utf-8") as fh:
            for line in fh:
                line = line.strip()
                if line == "":
                    sents += 1
                    continue
                parts = line.split("\t")
                if len(parts) >= 2:
                    tokens += 1
                    if parts[1].startswith("B-"):
                        etypes.add(parts[1][2:])
    tags_str = ", ".join(sorted(etypes))
    name = f"IndoLEM NER-{corpus[-2:].upper()}"
    print(f"â”‚  {name:22s} â”‚ Open Source  â”‚ {tokens:>10,} tok â”‚ {tags_str:14s} â”‚ NER  â”‚")

nusax_total = sum(len(v) for v in splits.values())
print(f"â”‚  NusaX-Senti (ind)      â”‚ Open Source  â”‚ {nusax_total:>10,}     â”‚ pos/neu/neg    â”‚ Sent â”‚")
print(f"â”‚  Google Maps Reviews    â”‚ Self-collect â”‚ ~est 5-10K â”‚ pos/neu/neg    â”‚ Sent â”‚")
print(f"â”‚  KBLI 2020 Taxonomy     â”‚ BPS Gov      â”‚ 1,810 codesâ”‚ Business cats  â”‚ Ref  â”‚")
print(f"â”‚  GeoBoundaries IDN      â”‚ HDX/OSM      â”‚ 514 ADM2   â”‚ Geometry       â”‚ Geo  â”‚")
print(f"â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜")

print("\nâœ… All public/open-source datasets verified and downloaded.")
print("ğŸ“‹ Self-collected data (Google Maps reviews) requires API access.")
print("ğŸ—ºï¸  Geospatial data (GeoBoundaries, OSM) available via HDX download.")
print("ğŸ“Š KBLI 2020 taxonomy available from BPS / oss.go.id.")
